{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1cf090",
   "metadata": {},
   "source": [
    "# Quesitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e65d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding Assesment: \n",
    "# Implement Processing JSON and CSV data with PySpark\n",
    "# Explain ETL (Extract, Transform, Load) with PySpark\n",
    "# Using Spark SQL - Creating databases, tables \n",
    "# Using Spark SQL - Transformations such as Filter, Join, Simple Aggregations, GroupBy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be19b1c9",
   "metadata": {},
   "source": [
    "#### Answer for question1 :  Implement Processing JSON and CSV data with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71f9e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing JSON and CSV data with PySpark\n",
    "\n",
    "# reading csv file\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark_coding\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da215f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+---+\n",
      "|Name|M1 Score|M2 Score|age|\n",
      "+----+--------+--------+---+\n",
      "|Alex|      62|      80| 20|\n",
      "|Brad|      45|      56| 19|\n",
      "|Joey|      85|      98| 21|\n",
      "|NULL|      54|      79| 20|\n",
      "|abhi|    NULL|    NULL| 20|\n",
      "+----+--------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#showing entire csv data\n",
    "read_csv_data = spark.read.csv(\"Marks_data.csv\",header = True, inferSchema = True )\n",
    "read_csv_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "916b3e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'M1 Score', 'M2 Score', 'age']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_csv_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50264f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cgpa: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- phonenumber: string (nullable = true)\n",
      " |-- rollno: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading json file data\n",
    "read_json_data = spark.read.json(\"sample_json.json\")\n",
    "read_json_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cc24d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----------+------+\n",
      "|cgpa|   name|phonenumber|rollno|\n",
      "+----+-------+-----------+------+\n",
      "|8.18|Abhiram| 9976770500|   580|\n",
      "+----+-------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_json_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1c809",
   "metadata": {},
   "source": [
    "### Answer for question2 :  Explain ETL (Extract, Transform, Load) with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3150449e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+----------+------+\n",
      "|  name| id|age|department|salary|\n",
      "+------+---+---+----------+------+\n",
      "| user1|  1| 25|Jr manager| 98000|\n",
      "| user2|  2| 30|sr manager|100000|\n",
      "| user3|  6| 35|sr manager|100000|\n",
      "| user4|  4| 32|      head| 70000|\n",
      "| user5|  1| 45|Jr manager| 60000|\n",
      "| user6|  6| 47|     head2| 45000|\n",
      "| user7|  5| 21|    worker| 25000|\n",
      "| user8|  1| 22|Jr manager| 50000|\n",
      "| user9| 10| 54|      lead| 45000|\n",
      "|user10| 59| 52|     lead2| 50000|\n",
      "|user11|  6| 25|     head2| 50000|\n",
      "|user12|  2| 27|sr manager| 70000|\n",
      "|user13| 59| 54|     lead2| 45000|\n",
      "|user14|  2| 25|sr manager| 70000|\n",
      "|user15|  1| 32|Jr manager| 50000|\n",
      "|user16|  3| 37|    worker| 25000|\n",
      "|user17| 74| 63|   Manager| 68000|\n",
      "|user18|  7| 25|      head| 45000|\n",
      "|user19| 10| 32| lvl2 head| 52000|\n",
      "|user20| 10| 32| lvl2 head| 52000|\n",
      "+------+---+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ETl process on a sample data set\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,concat,lit,floor,rand\n",
    "spark = SparkSession.builder.appName(\"pyspark_coding\").getOrCreate()\n",
    "\n",
    "read_etl = spark.read.csv(\"salary.csv\",header = True, inferSchema = True)\n",
    "read_etl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67ece3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+----------+------+---------+-------+\n",
      "|  name| id|age|department|salary|Total_sal|Avg_sal|\n",
      "+------+---+---+----------+------+---------+-------+\n",
      "| user1|  1| 25|Jr manager| 98000|    50032|  29724|\n",
      "| user2|  2| 30|sr manager|100000|    50015|  25605|\n",
      "| user3|  6| 35|sr manager|100000|    50037|  25137|\n",
      "| user4|  4| 32|      head| 70000|    50012|  29389|\n",
      "| user5|  1| 45|Jr manager| 60000|    50035|  32277|\n",
      "| user6|  6| 47|     head2| 45000|    50030|  29663|\n",
      "| user7|  5| 21|    worker| 25000|    50047|  26395|\n",
      "| user8|  1| 22|Jr manager| 50000|    50033|  31826|\n",
      "| user9| 10| 54|      lead| 45000|    50038|  32459|\n",
      "|user10| 59| 52|     lead2| 50000|    50038|  34986|\n",
      "+------+---+---+----------+------+---------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " #creating new column\n",
    "read_etl = read_etl.withColumn('Avg_sal',floor(lit(25000) + rand() * lit(10000)))\n",
    "read_etl.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "607c796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+----------+------+---------+-------+\n",
      "|  name| id|age|department|salary|Total_sal|Avg_sal|\n",
      "+------+---+---+----------+------+---------+-------+\n",
      "| user9| 10| 54|      lead| 45000|    50038|  32459|\n",
      "|user10| 59| 52|     lead2| 50000|    50038|  34986|\n",
      "|user13| 59| 54|     lead2| 45000|    50019|  25204|\n",
      "|user17| 74| 63|   Manager| 68000|    50047|  30134|\n",
      "+------+---+---+----------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_etl = read_etl.filter(col('age')>= 50)\n",
    "read_etl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91c3426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+----------+------+---------+-------+\n",
      "|  name| id|age|department|salary|Total_sal|Avg_sal|\n",
      "+------+---+---+----------+------+---------+-------+\n",
      "|user10| 59| 52|     lead2| 50000|    50038|  34986|\n",
      "| user9| 10| 54|      lead| 45000|    50038|  32459|\n",
      "|user13| 59| 54|     lead2| 45000|    50019|  25204|\n",
      "|user17| 74| 63|   Manager| 68000|    50047|  30134|\n",
      "+------+---+---+----------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_etl = read_etl.orderBy(\"age\")\n",
    "read_etl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1041558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#laoding the dataset into another file\n",
    "target_path = \"c:desktop\\salary_result.csv\"\n",
    "read_etl.write.csv(target_path, mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685ce58",
   "metadata": {},
   "source": [
    "### Answer to question3 : Using Spark SQL - Creating databases, tables  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a2a7a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  Name|  lang|\n",
      "+------+------+\n",
      "|  Abhi|python|\n",
      "|   ram|   c++|\n",
      "|cherry|  java|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating databases\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"pyspark_coding\").getOrCreate()\n",
    "\n",
    "# Create a new database\n",
    "database_name = \"coding_database\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "# using created database\n",
    "spark.sql(f\"USE {database_name}\")\n",
    "\n",
    "# Create a DataFrame using spark.sql\n",
    "data = [(\"Abhi\", \"python\"), (\"ram\", \"c++\"), (\"cherry\", \"java\")]\n",
    "columns = [\"Name\", \"lang\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Register the DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"Coding_table\")\n",
    "\n",
    "# Run a SQL query on the temporary table\n",
    "result = spark.sql(\"SELECT * FROM Coding_table\" )\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ec11aa",
   "metadata": {},
   "source": [
    "### Answer to question 4 :Using Spark SQL - Transformations such as Filter, Join, Simple Aggregations, GroupBy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1177e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,concat,lit,floor,rand\n",
    "spark = SparkSession.builder.appName(\"pyspark_coding\").getOrCreate()\n",
    "agg_data = spark.read.csv(\"salary.csv\",header = True, inferSchema = True)\n",
    "agg_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "682e246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+----------+------+\n",
      "|  name| id|age|department|salary|\n",
      "+------+---+---+----------+------+\n",
      "| user2|  2| 30|sr manager|100000|\n",
      "| user3|  6| 35|sr manager|100000|\n",
      "|user12|  2| 27|sr manager| 70000|\n",
      "|user14|  2| 25|sr manager| 70000|\n",
      "+------+---+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#performing filter \n",
    "\n",
    "filter_col = agg_data.filter(agg_data[\"department\"] == \"sr manager\")\n",
    "filter_col.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "14a7a120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------+\n",
      "|Name|  lang|    Role|\n",
      "+----+------+--------+\n",
      "|Abhi|python|Engineer|\n",
      "|arun|  java| Analyst|\n",
      "+----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#implementing joins\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"pyspark_coding\").getOrCreate()\n",
    "\n",
    "# Create two DataFrames\n",
    "data1 = [(\"Abhi\", \"python\"), (\"ram\", \"c++\"), (\"cherry\", \"java\"),(\"arun\", \"java\")]\n",
    "columns = [\"Name\", \"lang\"]\n",
    "df1 = spark.createDataFrame(data1, columns)\n",
    "\n",
    "data2 = [(\"Abhi\", \"Engineer\"), (\"arun\", \"Analyst\"), (\"kiran\", \"Manager\")]\n",
    "columns2 = [\"Name\", \"Role\"]\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Perform an inner join on the 'Name' column\n",
    "join_col = df1.join(df2, \"Name\", \"inner\")\n",
    "\n",
    "# Show the result\n",
    "join_col.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c9e9e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|max(salary)|\n",
      "+----------+-----------+\n",
      "|Jr manager|      98000|\n",
      "|      head|      70000|\n",
      "|sr manager|     100000|\n",
      "|     head2|      50000|\n",
      "| lvl2 head|      52000|\n",
      "|      lead|      45000|\n",
      "|   Manager|      68000|\n",
      "|    worker|      25000|\n",
      "|     lead2|      50000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggregate function: max()\n",
    "filter_col2 = agg_data.groupBy(\"department\").max(\"salary\")\n",
    "filter_col2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b99524c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|min(salary)|\n",
      "+----------+-----------+\n",
      "|Jr manager|      50000|\n",
      "|      head|      45000|\n",
      "|sr manager|      70000|\n",
      "|     head2|      45000|\n",
      "| lvl2 head|      52000|\n",
      "|      lead|      45000|\n",
      "|   Manager|      68000|\n",
      "|    worker|      25000|\n",
      "|     lead2|      45000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggregate function: min()\n",
    "filter_col3 = agg_data.groupBy(\"department\").min(\"salary\")\n",
    "filter_col3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e4e15962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|avg(salary)|\n",
      "+----------+-----------+\n",
      "|Jr manager|    64500.0|\n",
      "|      head|    57500.0|\n",
      "|sr manager|    85000.0|\n",
      "|     head2|    47500.0|\n",
      "| lvl2 head|    52000.0|\n",
      "|      lead|    45000.0|\n",
      "|   Manager|    68000.0|\n",
      "|    worker|    25000.0|\n",
      "|     lead2|    47500.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggregate function: avg()\n",
    "filter_col3 = agg_data.groupBy(\"department\").avg(\"salary\")\n",
    "filter_col3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2bbadd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Jr manager|     258000|\n",
      "|      head|     115000|\n",
      "|sr manager|     340000|\n",
      "|     head2|      95000|\n",
      "| lvl2 head|     104000|\n",
      "|      lead|      45000|\n",
      "|   Manager|      68000|\n",
      "|    worker|      50000|\n",
      "|     lead2|      95000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# aggregate function: sum()\n",
    "filter_col3 = agg_data.groupBy(\"department\").sum(\"salary\")\n",
    "filter_col3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "62d4a8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|avg(salary)|\n",
      "+----------+-----------+\n",
      "|Jr manager|    64500.0|\n",
      "|      head|    57500.0|\n",
      "|sr manager|    85000.0|\n",
      "|     head2|    47500.0|\n",
      "| lvl2 head|    52000.0|\n",
      "|      lead|    45000.0|\n",
      "|   Manager|    68000.0|\n",
      "|    worker|    25000.0|\n",
      "|     lead2|    47500.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggregate function: sum()\n",
    "filter_col3 = agg_data.groupBy(\"department\").mean(\"salary\")\n",
    "filter_col3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0cd03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f60c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c94fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a719e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04728ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd8ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc5b24b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92576b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b37c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_data.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
